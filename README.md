# ğŸ­ The Empathy Engine

**AI-Powered Text-to-Speech with Emotional Intelligence**

The Empathy Engine is a sophisticated service that bridges the gap between text-based sentiment and expressive, human-like audio output. It dynamically modulates vocal characteristics (rate, pitch, volume) of synthesized speech based on detected emotions, moving beyond monotonic TTS delivery to achieve emotional resonance.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Design Decisions](#design-decisions)
- [Emotion-to-Voice Mapping](#emotion-to-voice-mapping)
- [Project Structure](#project-structure)
- [Future Enhancements](#future-enhancements)

---

## ğŸ¯ Overview

### Problem Statement
Standard Text-to-Speech systems are functional but robotic, lacking the prosody, emotional range, and subtle vocal cues that build trust and rapport in human communication. This emotional disconnect affects user experience in AI-driven customer interactions.

### Solution
The Empathy Engine detects emotion in text and programmatically modulates TTS vocal parameters to create speech that sounds genuinely enthusiastic, patient, frustrated, or calmâ€”matching the emotional context of the content.

### Core Pipeline
```
Text Input â†’ Emotion Detection â†’ Voice Parameter Mapping â†’ TTS Synthesis â†’ Emotional Audio Output
```

---

## âœ¨ Features

### Core Requirements (Implemented)
âœ… **Text Input**: Accepts string input via CLI and web API  
âœ… **Emotion Detection**: Classifies text into 7 distinct emotions (joy, anger, sadness, fear, surprise, disgust, neutral)  
âœ… **Vocal Parameter Modulation**: Modulates 3 parameters (rate, pitch, volume)  
âœ… **Emotion-to-Voice Mapping**: Clear, demonstrable logic mapping emotions to vocal configurations  
âœ… **Audio Output**: Generates playable `.wav` files  

### Bonus Features (Implemented)
ğŸŒŸ **Granular Emotions**: 7 nuanced emotional states beyond basic positive/negative/neutral  
ğŸŒŸ **Intensity Scaling**: Emotion confidence scores scale the degree of modulation  
ğŸŒŸ **Web Interface**: Premium UI with text input and embedded audio player  
ğŸŒŸ **REST API**: FastAPI backend with comprehensive endpoints  

---

## ğŸ—ï¸ Architecture

### Technology Stack

| Component | Technology | Justification |
|-----------|-----------|---------------|
| **Emotion Detection** | `transformers` + `j-hartmann/emotion-english-distilroberta-base` | State-of-the-art emotion classification with 7 emotion categories |
| **TTS Engine** | `edge-tts` | High-quality Microsoft Neural voices, no API key required, programmatic parameter control |
| **Backend** | `FastAPI` | Modern, async Python framework with auto-generated API docs |
| **Frontend** | HTML/CSS/JS | Lightweight, dependency-free web interface |

### System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Input â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Emotion Detector           â”‚
â”‚  (Transformer Model)        â”‚
â”‚  Returns: emotion + score   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Voice Parameter Mapper     â”‚
â”‚  Maps emotion â†’ rate/pitch  â”‚
â”‚  Scales by confidence       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TTS Engine (edge-tts)      â”‚
â”‚  Applies modulation         â”‚
â”‚  Generates .wav file        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Output + Analysis    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Requirements

### System Requirements
- **Python**: 3.8 or higher
- **Internet Connection**: Required for edge-tts synthesis
- **OS**: Windows, macOS, or Linux

### Dependencies
See [requirements.txt](requirements.txt) for complete list:
- `fastapi` - Web framework
- `edge-tts` - Neural TTS engine
- `transformers` - Emotion detection model
- `torch` - PyTorch for model inference
- `uvicorn` - ASGI server

---

## ğŸš€ Installation

### Step 1: Clone the Repository
```bash
git clone <your-repo-url>
cd Empathy.Engine
```

### Step 2: Create Virtual Environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

**Note**: First run will download the emotion detection model (~250MB). This is a one-time download.

---

## ğŸ’» Usage

### Option 1: Web Interface (Recommended)

1. **Start the server**:
```bash
python main.py
```

2. **Open your browser**:
```
http://localhost:8000
```

3. **Use the interface**:
   - Enter text in the input area
   - Click "ğŸ¤ Generate Speech" to create emotional audio
   - Click "ğŸ” Analyze Emotion Only" to see emotion detection without synthesis
   - Click example texts to quickly test different emotions

### Option 2: API Integration

Start the server:
```bash
python main.py
```

**Synthesize Speech:**
```bash
curl -X POST "http://localhost:8000/synthesize" \
  -H "Content-Type: application/json" \
  -d '{"text": "This is absolutely amazing! I am so excited!"}'
```

**Analyze Emotion:**
```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{"text": "I am feeling really frustrated right now."}'
```

### Option 3: Python Integration

```python
from emotion_detector import get_emotion_detector
from tts_engine import EmotionalTTSEngine, EmpathyEngine
import asyncio

# Initialize
detector = get_emotion_detector()
tts = EmotionalTTSEngine()
engine = EmpathyEngine(detector, tts)

# Process text
async def generate_speech():
    result = await engine.process_text(
        "This is the best news I've heard all day!"
    )
    print(f"Emotion: {result['emotion']['label']}")
    print(f"Audio saved: {result['audio_file']}")

asyncio.run(generate_speech())
```

---

## ğŸ“š API Documentation

### Interactive API Docs
Once the server is running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Key Endpoints

#### `POST /synthesize`
Generate emotionally-modulated speech from text.

**Request:**
```json
{
  "text": "I can't believe this happened! This is incredible!",
  "voice": "en-US-AriaNeural"
}
```

**Response:**
```json
{
  "text": "I can't believe this happened! This is incredible!",
  "emotion": {
    "label": "surprise",
    "confidence": 0.8542,
    "description": "Excited and astonished",
    "all_scores": [...]
  },
  "voice_modulation": {
    "rate": "+10%",
    "pitch": "+60Hz",
    "volume": "+15%"
  },
  "audio_file": "empathy_output_surprise_20260223_143022.wav",
  "download_url": "/audio/empathy_output_surprise_20260223_143022.wav"
}
```

#### `POST /analyze`
Analyze emotion without generating audio.

#### `GET /emotions`
List all supported emotions and their vocal characteristics.

#### `GET /audio/{filename}`
Download/stream generated audio files.

---

## ğŸ§  Design Decisions

### 1. Why edge-tts?
**Decision**: Use Microsoft Edge TTS instead of alternatives like gTTS or pyttsx3

**Rationale**:
- âœ… High-quality neural voices (Microsoft's latest TTS technology)
- âœ… Programmatic control over rate, pitch, and volume
- âœ… No API key or authentication required
- âœ… Supports multiple languages and voices
- âŒ Requires internet connection (acceptable tradeoff for quality)

**Alternatives Considered**:
- `pyttsx3`: Offline but robotic quality, limited emotional expression
- `gTTS`: Good quality but no pitch/rate control
- `Google Cloud TTS`: Requires API key and billing

### 2. Why j-hartmann/emotion-english-distilroberta-base?
**Decision**: Use this specific transformer model for emotion detection

**Rationale**:
- âœ… 7 distinct emotion categories (vs. 3 for basic sentiment)
- âœ… State-of-the-art accuracy on emotion classification
- âœ… Based on RoBERTa (robust BERT variant)
- âœ… Actively maintained and well-documented
- âœ… Returns confidence scores for intensity scaling

**Alternatives Considered**:
- `TextBlob/VADER`: Too simplistic (only positive/negative/neutral)
- Custom training: Time-consuming, requires labeled dataset

### 3. Why FastAPI?
**Decision**: Use FastAPI over Flask or other frameworks

**Rationale**:
- âœ… Native async support (critical for I/O-bound TTS operations)
- âœ… Auto-generated OpenAPI documentation
- âœ… Type validation with Pydantic
- âœ… Modern Python features (type hints, async/await)
- âœ… High performance

### 4. Intensity Scaling Logic
**Decision**: Scale vocal modulation based on emotion confidence score

**Implementation**:
```python
scaled_modulation = base_modulation * confidence_score
```

**Example**:
- "This is good" â†’ joy (70% confidence) â†’ pitch +35Hz (scaled from +50Hz)
- "This is AMAZING!" â†’ joy (95% confidence) â†’ pitch +47.5Hz (nearly full +50Hz)

This creates natural variation where stronger emotions produce more pronounced vocal changes.

---

## ğŸ¨ Emotion-to-Voice Mapping

The core logic mapping emotions to vocal parameters is defined in `config.py`:

| Emotion | Rate | Pitch | Volume | Reasoning |
|---------|------|-------|--------|-----------|
| **Joy** | +15% | +50Hz | +10% | Faster, higher, louder = enthusiastic |
| **Anger** | +10% | +20Hz | +20% | Intense, forceful, louder |
| **Sadness** | -20% | -50Hz | -10% | Slower, lower, quieter = melancholic |
| **Fear** | +20% | +80Hz | -5% | Rapid, high-pitched = anxious |
| **Surprise** | +10% | +60Hz | +15% | Quick, high, loud = shocked |
| **Disgust** | -10% | -30Hz | +5% | Measured, lower = disapproving |
| **Neutral** | +0% | +0Hz | +0% | Standard professional tone |

### Design Principles
1. **Rate**: Emotional arousal correlates with speech speed (excited â†’ faster, sad â†’ slower)
2. **Pitch**: Positive emotions raise pitch, negative emotions lower it
3. **Volume**: High-energy emotions are louder, subdued emotions are quieter
4. **Intensity**: Confidence score scales all parameters proportionally

---

## ğŸ“ Project Structure

```
Empathy.Engine/
â”‚
â”œâ”€â”€ main.py                 # FastAPI application & endpoints
â”œâ”€â”€ emotion_detector.py     # Emotion detection module
â”œâ”€â”€ tts_engine.py          # TTS engine & EmpathyEngine orchestrator
â”œâ”€â”€ config.py              # Configuration & emotion-to-voice mapping
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ .gitignore            # Git ignore patterns
â”‚
â”œâ”€â”€ static/               # Web UI assets
â”‚   â””â”€â”€ index.html       # Premium web interface
â”‚
â””â”€â”€ output/              # Generated audio files (auto-created)
    â””â”€â”€ *.wav           # Emotional speech outputs
```

---

## ğŸ”¬ Testing Examples

### Test Cases for Evaluation

| Text | Expected Emotion | Expected Modulation |
|------|-----------------|---------------------|
| "This is absolutely amazing! I'm so excited!" | Joy | +15% rate, +50Hz pitch |
| "I'm really frustrated with this situation." | Anger | +10% rate, +20Hz pitch |
| "I feel so sad and alone right now." | Sadness | -20% rate, -50Hz pitch |
| "What?! I can't believe this just happened!" | Surprise | +10% rate, +60Hz pitch |
| "The quarterly report shows steady progress." | Neutral | +0% rate, +0Hz pitch |

**Try these via the web interface or API to validate the emotion-to-voice mapping.**

---

## ğŸš€ Future Enhancements

### Potential Improvements
1. **SSML Support**: Advanced control over emphasis, pauses, and phonetics
2. **Multi-language**: Extend to non-English languages
3. **Voice Selection**: Allow users to choose from multiple voices
4. **Custom Emotion Mapping**: User-configurable emotion-to-voice mappings
5. **Batch Processing**: Process multiple texts simultaneously
6. **Emotion Mixing**: Blend parameters when multiple emotions are detected
7. **Real-time Streaming**: Stream audio as it's generated
8. **Offline Mode**: Add fallback to pyttsx3 for offline usage

---

## ğŸ“ Notes for Judges

### Requirements Compliance

âœ… **Text Input**: Multiple input methods (API, web UI)  
âœ… **Emotion Detection**: 7 distinct emotional categories  
âœ… **Vocal Parameter Modulation**: 3 parameters (rate, pitch, volume)  
âœ… **Emotion-to-Voice Mapping**: Clearly defined in `config.py` with documented reasoning  
âœ… **Audio Output**: High-quality `.wav` files with Microsoft Neural voices  

### Distinguishing Features

1. **Production-Ready Code**: Clean architecture, type hints, comprehensive logging
2. **Intensity Scaling**: Confidence-based modulation (bonus feature)
3. **Premium Web UI**: Professional, polished interface
4. **Comprehensive Documentation**: Extensive README with design rationale
5. **API-First Design**: FastAPI with auto-generated OpenAPI docs

### Running the Demo
```bash
# Single command to see it in action:
python main.py

# Then visit http://localhost:8000
```

---

## ğŸ“„ License

This project is created for hackathon/educational purposes.

---

## ğŸ‘¥ Author

Built with â¤ï¸ for The Empathy Engine Challenge

---

**Questions or issues?** Check the `/docs` endpoint for interactive API documentation or review the code comments for implementation details.
